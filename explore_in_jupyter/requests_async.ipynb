{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "setup"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# TODO: Move the filters and the notifications to aws kinesis and lambda. Fill stream name below. Check the record weights in kinesis\r\n",
    "# TODO: Place this code in a cloudfront yaml file\r\n",
    "# TODO: Optimize json sizes and code speed\r\n",
    "# TODO: Increase amount of shards and add proxies and asynchronous code\r\n",
    "# TODO: Feed the data to aws s3 and then aws sagemaker\r\n",
    "import json\r\n",
    "import time\r\n",
    "import random\r\n",
    "import uuid\r\n",
    "import sys\r\n",
    "import os\r\n",
    "import asyncio\r\n",
    "from collections import Counter\r\n",
    "\r\n",
    "import aiohttp\r\n",
    "import urllib3\r\n",
    "import requests\r\n",
    "import pandas as pd\r\n",
    "import boto3\r\n",
    "import discord\r\n",
    "from dotenv import load_dotenv\r\n",
    "\r\n",
    "\r\n",
    "load_dotenv()\r\n",
    "SCRAPER_API_KEY = os.environ[\"KEY\"]\r\n",
    "\r\n",
    "query = \"\"\"\r\n",
    "query GetAxieLatest($auctionType: AuctionType, $criteria: AxieSearchCriteria, $from: Int, $sort: SortBy, $size: Int, $owner: String) {\r\n",
    "  axies(auctionType: $auctionType, criteria: $criteria, from: $from, sort: $sort, size: $size, owner: $owner) {\r\n",
    "    results {\r\n",
    "      ...AxieBrief\r\n",
    "    }\r\n",
    "  }\r\n",
    "}\r\n",
    "\r\n",
    "fragment AxieBrief on Axie {\r\n",
    "  id\r\n",
    "  breedCount\r\n",
    "  image\r\n",
    "  auction {\r\n",
    "    currentPrice\r\n",
    "    currentPriceUSD\r\n",
    "  }\r\n",
    "  battleInfo {\r\n",
    "    banned\r\n",
    "  }\r\n",
    "  parts {\r\n",
    "    name\r\n",
    "    class\r\n",
    "    type\r\n",
    "    specialGenes\r\n",
    "  }\r\n",
    "}\r\n",
    "\"\"\"\r\n",
    "\r\n",
    "\r\n",
    "def variables(fromm):\r\n",
    "    return {\r\n",
    "        \"from\": fromm,\r\n",
    "        \"size\": 100,\r\n",
    "        \"sort\": \"PriceAsc\",\r\n",
    "        \"auctionType\": \"Sale\",\r\n",
    "        \"owner\": None,\r\n",
    "        \"criteria\": {\r\n",
    "            \"region\": None,\r\n",
    "            \"parts\": None,\r\n",
    "            \"bodyShapes\": None,\r\n",
    "            \"classes\": None,\r\n",
    "            \"stages\": None,\r\n",
    "            \"numMystic\": None,\r\n",
    "            \"pureness\": None,\r\n",
    "            \"title\": None,\r\n",
    "            \"breedable\": None,\r\n",
    "            \"breedCount\": None,\r\n",
    "            \"hp\": [],\r\n",
    "            \"skill\": [],\r\n",
    "            \"speed\": [],\r\n",
    "            \"morale\": [],\r\n",
    "        },\r\n",
    "    }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "more setup"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "url = \"https://axieinfinity.com/graphql-server-v2/graphql\"\r\n",
    "proxies = {\r\n",
    "    \"http\": f\"http://scraperapi:{SCRAPER_API_KEY}@proxy-server.scraperapi.com:8001\"  # https to hit the proxy\r\n",
    "}\r\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\r\n",
    "# client = boto3.client('kinesis',region_name='eu-west-1')\r\n",
    "partition_key = str(uuid.uuid4())\r\n",
    "DISCORD_WEBHOOK_URL = os.environ[\"DISCORD_WEBHOOK_URL\"]\r\n",
    "webhook = discord.Webhook.from_url(\r\n",
    "    DISCORD_WEBHOOK_URL, adapter=discord.RequestsWebhookAdapter()\r\n",
    ")\r\n",
    "\r\n",
    "# Filter by build\r\n",
    "leaderboard = pd.read_csv(\"../leaderboard/leaderboard.csv\")\r\n",
    "leaderboard = (\r\n",
    "    leaderboard[[\"Back\", \"Mouth\", \"Horn\", \"Tail\"]].drop_duplicates().values.tolist()\r\n",
    ")\r\n",
    "BEST_BUILDS = set(tuple(x) for x in leaderboard)\r\n",
    "# DonÂ´t retrieve the same nft twice\r\n",
    "bargains = set()\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "aoihttp without gather"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "results = []\r\n",
    "proxy = f\"http://scraperapi:{SCRAPER_API_KEY}@proxy-server.scraperapi.com:8001\"  # https to hit the proxy\r\n",
    "async def get_symbols():\r\n",
    "    timeout = aiohttp.ClientTimeout(total=10)\r\n",
    "    async with aiohttp.ClientSession(timeout=timeout) as session:\r\n",
    "         for i in range(0, 500, 100):\r\n",
    "            print(f'Starting on axie {i}')\r\n",
    "            try:\r\n",
    "                response = await asyncio.create_task(session.post(url, \r\n",
    "                    json={\"query\": query, \"variables\": variables(fromm=i)}, \r\n",
    "                    proxy=proxy,\r\n",
    "                    # timeout=10,\r\n",
    "                    ssl=False))\r\n",
    "            except asyncio.TimeoutError:\r\n",
    "                print('Timeout fail')\r\n",
    "            else: \r\n",
    "                try:\r\n",
    "                    results.append(await response.json())\r\n",
    "                except:\r\n",
    "                    print('Fail')\r\n",
    "\r\n",
    "await get_symbols()\r\n",
    "print(results)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "aoihttp with gather"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "results = []\r\n",
    "def get_tasks(session):\r\n",
    "    tasks = []\r\n",
    "    for i in range(0, 200, 100):\r\n",
    "        tasks.append(asyncio.create_task(session.post(url, \r\n",
    "                    json={\"query\": query, \"variables\": variables(fromm=i)}, \r\n",
    "                    proxy=proxy,\r\n",
    "                    ssl=False))) #create_task adds to the event loop\r\n",
    "    return tasks\r\n",
    "async def run_tasks():\r\n",
    "    timeout = aiohttp.ClientTimeout(total=20)\r\n",
    "    async with aiohttp.ClientSession(timeout=timeout) as session:\r\n",
    "        tasks = get_tasks(session)\r\n",
    "        responses = await asyncio.gather(*tasks,return_exceptions=True) # gather would add any tasks not in the event loop to the loop\r\n",
    "        for response in responses:\r\n",
    "            if isinstance(response, asyncio.TimeoutError):\r\n",
    "                print('TimeoutError')\r\n",
    "            else:\r\n",
    "                if response.status == 200:\r\n",
    "                    try:\r\n",
    "                        data = await response.json()\r\n",
    "                        results.append(data[\"data\"][\"axies\"][\"results\"])\r\n",
    "                    except:\r\n",
    "                        print('Failed to append')\r\n",
    "                else:\r\n",
    "                    print(f\"Unexpected status code returned: {response.status}\")         \r\n",
    "    return results\r\n",
    "\r\n",
    "start = time.time()\r\n",
    "jsn_data = await run_tasks() # asyncio.run(run_tasks())\r\n",
    "end = time.time()\r\n",
    "print(f\"Time to make API calls with tasks, it took: {end - start}\")\r\n",
    "print(jsn_data)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('venv': venv)"
  },
  "interpreter": {
   "hash": "7c13fb0347b60adcd474a811c9df209bcf6f636761ad04fac099312220478efe"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}